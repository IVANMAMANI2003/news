# ğŸ“° Sistema de Scraping de Noticias Automatizado

Sistema completo de recolecciÃ³n automÃ¡tica de noticias de mÃºltiples fuentes peruanas con almacenamiento en PostgreSQL y ejecuciÃ³n recursiva.

## ğŸ¯ CaracterÃ­sticas

- **Scraping de 4 fuentes de noticias**:
  - Diario Sin Fronteras
  - Los Andes
  - Pachamama Radio
  - Puno Noticias
- **Base de datos PostgreSQL** con esquema optimizado
- **EjecuciÃ³n recursiva** cada hora automÃ¡ticamente (Celery Beat)
- **Colas de tareas** con Celery + Redis (worker y beat)
- **Monitoreo de tareas** con Flower
- **GeneraciÃ³n de archivos** CSV y JSON por fuente y consolidados
- **Sistema de logging** completo
- **Preparado para AWS** con Docker y scripts de despliegue
- **Interfaz web** para monitoreo

## â˜ï¸ Repositorio

- GitHub: [`IVANMAMANI2003/news`](https://github.com/IVANMAMANI2003/news.git)

## ğŸ³ EjecuciÃ³n en Docker (recomendada)

```bash
# Construir e iniciar todos los servicios (usar sudo en AWS)
sudo docker-compose up -d --build

# Ver estado de contenedores
sudo docker ps

# Logs del worker Celery
sudo docker logs -f news_celery_worker | sed -u 's/^/[CELERY-WORKER] /'

# Logs del beat Celery
sudo docker logs -f news_celery_beat | sed -u 's/^/[CELERY-BEAT] /'

# Logs del scraper (ejecuciÃ³n one-shot)
sudo docker logs -f news_scraper | sed -u 's/^/[SCRAPER] /'

# Monitoreo Flower
# URL: http://<IP-SERVIDOR>:5555
```

Servicios incluidos en `docker-compose.yml`:
- `postgres`: Base de datos
- `redis`: Broker/Backend Celery
- `scraper`: ejecuciÃ³n Ãºnica de scraping (Ãºtil para pruebas)
- `celery_worker`: procesador de tareas
- `celery_beat`: programador (cada hora)
- `flower`: monitoreo de tareas en `:5555`

## ğŸ” ProgramaciÃ³n con Celery

- Tarea periÃ³dica cada hora: `tasks.scrape_all_sources`
- Ejecutar manualmente dentro de contenedor worker:
```bash
sudo docker exec -it news_celery_worker bash -lc "python - <<'PY'
from tasks import scrape_all_sources
r = scrape_all_sources.delay()
print('Task sent:', r.id)
PY"
```

## ğŸ—„ï¸ Base de datos y verificaciÃ³n

```bash
# Ingresar a PostgreSQL en contenedor
echo "SELECT fuente, COUNT(*), MAX(fecha_extraccion) FROM noticias GROUP BY fuente ORDER BY 2 DESC;" \
  | sudo docker exec -i news_postgres psql -U postgres -d news_scraping
```

## ğŸ§ª Pruebas rÃ¡pidas

```bash
# Ejecutar scraping one-shot (fuera del schedule)
sudo docker run --rm --network host -v $(pwd)/data:/app/data -v $(pwd)/logs:/app/logs \
  -e DB_HOST=localhost -e DB_USER=postgres -e DB_PASSWORD=123456 \
  $(sudo docker build -q .) python scheduler.py --mode once
```

## ğŸ” Recomendaciones AWS

- Anteponer `sudo` a todos los comandos del sistema: `sudo docker ...`, `sudo systemctl ...`
- Abrir puertos: 22, 80, 5432, 5555
- Monitorear logs en tiempo real:
```bash
sudo docker logs -f news_celery_worker
sudo docker logs -f news_celery_beat
sudo docker logs -f news_scraper
```
- Ver tareas en Flower: `http://<ip>:5555`

## ğŸ“œ Tareas Celery

Archivo: `tasks.py`
- `scrape_all_sources`: ejecuta scraping y guarda en PostgreSQL
- `scrape_single_source(<source_key>)`: procesa una sola fuente

## ğŸ“ Archivos relevantes

- `docker-compose.yml`: orquestaciÃ³n (Postgres, Redis, Worker, Beat, Flower)
- `tasks.py`: tareas Celery + beat schedule
- `news_scraper_manager.py`: orquesta scrapers + BD + files
- `database.py`: conexiÃ³n/DDL/insert en PostgreSQL
- `base_scraper.py` y `scrapers/*`: scrapers por sitio

## ğŸ—„ï¸ Esquema de Base de Datos

```sql
CREATE TABLE noticias (
    id SERIAL PRIMARY KEY,
    titulo TEXT,
    fecha TIMESTAMP,
    hora TIME,
    resumen TEXT,
    contenido TEXT,
    categoria VARCHAR(100),
    autor VARCHAR(200),
    tags TEXT,
    url TEXT UNIQUE,
    fecha_extraccion TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    link_imagenes TEXT,
    fuente VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ğŸ§­ Operaciones comunes (AWS)

```bash
# Construir y levantar todo
sudo docker-compose up -d --build

# Verificar que Beat estÃ© programando
sudo docker logs -f news_celery_beat | grep -E "Scheduler|scrape_all_sources"

# Verificar inserciones en BD
echo "SELECT COUNT(*) FROM noticias;" | sudo docker exec -i news_postgres psql -U postgres -d news_scraping

# Reiniciar servicios
sudo docker-compose restart
```

## ğŸ“‹ Requisitos

- Python 3.9+
- PostgreSQL 13+
- Docker (opcional)
- Ubuntu/Debian (para despliegue AWS)

## ğŸš€ InstalaciÃ³n Local

### 1. Clonar el repositorio
```bash
git clone <repository-url>
cd news-scraping
```

### 2. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 3. Configurar PostgreSQL
```bash
# Instalar PostgreSQL
sudo apt-get install postgresql postgresql-contrib

# Crear base de datos
sudo -u postgres psql
CREATE DATABASE news_scraping;
CREATE USER postgres WITH PASSWORD '123456';
GRANT ALL PRIVILEGES ON DATABASE news_scraping TO postgres;
\q
```

### 4. Configurar variables de entorno
```bash
cp .env.example .env
# Editar .env con sus configuraciones
```

### 5. Ejecutar el sistema
```bash
# Modo interactivo
python main.py

# Modo programado (cada hora)
python scheduler.py --mode schedule --interval 1

# Una sola ejecuciÃ³n
python scheduler.py --mode once
```

## ğŸ³ InstalaciÃ³n con Docker

### 1. Usar Docker Compose
```bash
# Iniciar todos los servicios
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener servicios
docker-compose down
```

### 2. Usar Docker individual
```bash
# Construir imagen
docker build -t news-scraper .

# Ejecutar con PostgreSQL externo
docker run -d \
  --name news-scraper \
  -e DB_HOST=host.docker.internal \
  -e DB_PASSWORD=123456 \
  -v $(pwd)/data:/app/data \
  news-scraper
```

## â˜ï¸ Despliegue en AWS

### 1. Preparar instancia EC2
- Instancia Ubuntu 20.04 LTS o superior
- MÃ­nimo 2GB RAM, 20GB almacenamiento
- Puertos abiertos: 22 (SSH), 80 (HTTP), 5432 (PostgreSQL)

### 2. Ejecutar script de despliegue
```bash
# Subir archivos a la instancia
scp -r . ubuntu@your-instance-ip:/home/ubuntu/

# Conectar a la instancia
ssh ubuntu@your-instance-ip

# Ejecutar script de despliegue
chmod +x deploy_aws.sh
./deploy_aws.sh
```

### 3. Verificar instalaciÃ³n
```bash
# Ver estado de servicios
sudo systemctl status news-scraper

# Ver logs
journalctl -u news-scraper -f

# Monitorear sistema
./monitor.sh
```

## ğŸ“Š Uso del Sistema

### Modo Interactivo
```bash
python main.py
```

MenÃº disponible:
1. Ejecutar scraping completo
2. Ejecutar fuente especÃ­fica
3. Generar archivos consolidados
4. Mostrar estadÃ­sticas
5. Iniciar modo programado
6. Salir

### Modo Programado
```bash
# Cada hora
python scheduler.py --mode schedule --interval 1

# Cada 6 horas
python scheduler.py --mode schedule --interval 6

# Una sola ejecuciÃ³n
python scheduler.py --mode once
```

### Fuente EspecÃ­fica
```bash
python scheduler.py --mode once --source diario_sin_fronteras
```

## ğŸ“ Estructura de Archivos

```
news-scraping/
â”œâ”€â”€ main.py                          # Script principal
â”œâ”€â”€ scheduler.py                     # Programador de tareas
â”œâ”€â”€ news_scraper_manager.py          # Gestor principal
â”œâ”€â”€ database.py                      # Manejador de BD
â”œâ”€â”€ base_scraper.py                  # Clase base para scrapers
â”œâ”€â”€ config.py                        # ConfiguraciÃ³n
â”œâ”€â”€ requirements.txt                 # Dependencias
â”œâ”€â”€ scrapers/                        # Scrapers especÃ­ficos
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ diario_sin_fronteras_scraper.py
â”‚   â”œâ”€â”€ los_andes_scraper.py
â”‚   â”œâ”€â”€ pachamama_scraper.py
â”‚   â””â”€â”€ puno_noticias_scraper.py
â”œâ”€â”€ data/                           # Archivos generados
â”œâ”€â”€ Dockerfile                      # Imagen Docker
â”œâ”€â”€ docker-compose.yml              # OrquestaciÃ³n
â”œâ”€â”€ deploy_aws.sh                   # Script de despliegue AWS
â”œâ”€â”€ nginx.conf                      # ConfiguraciÃ³n web
â””â”€â”€ README.md                       # Este archivo
```

## ğŸ—„ï¸ Esquema de Base de Datos

```sql
CREATE TABLE noticias (
    id SERIAL PRIMARY KEY,
    titulo TEXT,
    fecha TIMESTAMP,
    hora TIME,
    resumen TEXT,
    contenido TEXT,
    categoria VARCHAR(100),
    autor VARCHAR(200),
    tags TEXT,
    url TEXT UNIQUE,
    fecha_extraccion TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    link_imagenes TEXT,
    fuente VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ğŸ“ˆ Monitoreo

### Logs del Sistema
```bash
# Ver logs en tiempo real
tail -f scraper.log

# Ver logs del servicio
journalctl -u news-scraper -f

# Ver logs de Docker
docker-compose logs -f scraper
```

### EstadÃ­sticas
```bash
# Script de monitoreo
./monitor.sh

# Consultas SQL directas
psql -h localhost -U postgres -d news_scraping
```

### Interfaz Web
- URL: `http://your-server-ip`
- Archivos de datos: `http://your-server-ip/data/`
- API de estado: `http://your-server-ip/api/stats`

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno
```bash
# Base de datos
DB_HOST=localhost
DB_PORT=5432
DB_NAME=news_scraping
DB_USER=postgres
DB_PASSWORD=123456

# Logging
LOG_LEVEL=INFO

# AWS (para despliegue)
AWS_REGION=us-east-1
AWS_INSTANCE_ID=your-instance-id
```

### ConfiguraciÃ³n de Scraping
Editar `config.py` para ajustar:
- Delays entre requests
- NÃºmero de workers
- LÃ­mites de pÃ¡ginas
- Patrones de URLs

## ğŸ› ï¸ Mantenimiento

### Respaldos AutomÃ¡ticos
```bash
# Ejecutar respaldo manual
./backup.sh

# Los respaldos se ejecutan automÃ¡ticamente cada dÃ­a a las 2 AM
```

### Limpieza de Archivos
```bash
# Limpiar archivos antiguos (mÃ¡s de 30 dÃ­as)
find data/ -name "*.csv" -mtime +30 -delete
find data/ -name "*.json" -mtime +30 -delete
```

### ActualizaciÃ³n del Sistema
```bash
# Detener servicios
sudo systemctl stop news-scraper

# Actualizar cÃ³digo
git pull origin main

# Reiniciar servicios
sudo systemctl start news-scraper
```

## ğŸ› SoluciÃ³n de Problemas

### Error de ConexiÃ³n a BD
```bash
# Verificar PostgreSQL
sudo systemctl status postgresql

# Verificar conexiÃ³n
psql -h localhost -U postgres -d news_scraping
```

### Error de Permisos
```bash
# Dar permisos al usuario
sudo chown -R $USER:$USER /opt/news_scraping
chmod +x *.sh
```

### Error de Memoria
```bash
# Verificar uso de memoria
free -h
htop

# Ajustar lÃ­mites en config.py
```

## ğŸ“ Soporte

Para problemas o preguntas:
1. Revisar logs del sistema
2. Verificar configuraciÃ³n de red
3. Comprobar estado de servicios
4. Consultar este README

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver archivo LICENSE para mÃ¡s detalles.

---

**Desarrollado para la recolecciÃ³n automatizada de noticias peruanas** ğŸ‡µğŸ‡ª
